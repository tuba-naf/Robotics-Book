[
  {
    "filename": "assessments.md",
    "content": "---\nid: assessments\ntitle: Assessments\n---\n\n# Assessments\n\nThis section details the assessment methods for the course.\n\n# What You’ll Build and Be Graded On\n\nThroughout the course, you’ll complete hands-on projects that build progressively toward your **capstone: a fully autonomous humanoid robot**. Each assessment helps you apply what you’ve learned and ensures you gain practical, real-world skills.\n\n---\n\n## ROS 2 Package Development Project\n\n- Build a basic robot control package connecting **sensors → actuators → decision logic**.  \n- Learn to modularize code using **nodes, topics, and actions**.  \n- Test and debug individual modules before integrating into larger systems.  \n\nThink of it like **assembling a car engine**: each part must work independently before the whole system runs smoothly.\n\n---\n\n## Gazebo Simulation Task\n\n- Make your robot navigate a virtual environment.  \n- Avoid obstacles, detect objects, and reach designated targets.  \n- Apply your understanding of physics simulation and sensor data.  \n\nThis is equivalent to **flight training in a simulator** — you can make mistakes safely without damaging real hardware.\n\n---\n\n## Isaac Perception Pipeline\n\n- Build a perception pipeline using **Isaac Sim and Isaac ROS**.  \n- Implement **VSLAM** for mapping and navigation.  \n- Enable your robot to recognize objects, plan paths, and move intelligently.  \n\nImagine teaching a person to **explore a new house** and remember where furniture and obstacles are, all while walking around safely.\n\n---\n\n## Capstone Project — Autonomous Humanoid Robot\n\nBy the end of the course, you’ll integrate everything into a **working humanoid**:\n\n- Receives **voice commands** from a human.  \n- Uses perception to **identify objects and obstacles**.  \n- Plans a path and **navigates complex environments**.  \n- Grasps and manipulates objects with precision.  \n- Executes multi-step tasks autonomously.  \n\nIt’s like giving your robot a **job in the real world** — it listens, thinks, and acts independently.\n\n---\n\n## Why These Assessments Matter\n\n- Reinforce your understanding of **ROS 2, simulation, AI perception, and motion control**.  \n- Provide incremental **hands-on experience** before tackling the capstone.  \n- Build confidence and practical skills that transfer to real-world robotics projects.  \n\n---\n\n## Key Resources\n\n- [ROS 2 Tutorials](https://docs.ros.org/en/rolling/Tutorials.html) — For building packages and nodes  \n- [Gazebo Tutorials](http://gazebosim.org/tutorials) — Simulation exercises and environment setup  \n- [NVIDIA Isaac Sim Docs](https://developer.nvidia.com/isaac-sim) — Perception pipelines and navigation examples  \n- [OpenAI Whisper & GPT API](https://platform.openai.com/docs/) — Voice recognition and language-to-action integration  \n- Example GitHub repositories with ROS 2 + Isaac demos for humanoid robots\n\n"
  },
  {
    "filename": "cloud-lab.md",
    "content": "---\nid: cloud-lab\ntitle: Cloud Lab\n---\n\n# Cloud Lab\n\nThis section provides information about the cloud lab setup.\n# Using the Cloud to Train and Test Robots\n\nNot everyone has access to a high-end workstation with an RTX GPU. The **cloud lab option** lets you run simulations and train AI models on remote servers, then deploy them locally to your robot.\n\n---\n\n## 1. Cloud Workstations — Your Remote Robot Gym\n\nCloud instances (AWS, Azure, or NVIDIA Omniverse Cloud) can:\n\n- Run **NVIDIA Isaac Sim** and Gazebo remotely  \n- Train AI models without requiring an expensive local PC  \n- Handle heavy computation for perception, SLAM, and reinforcement learning\n\nThink of this as sending your robot to a **virtual university**: it can practice and learn without needing a local campus.\n\n---\n\n## 2. Local Edge Deployment — Bringing the Robot Home\n\nEven though your AI trains in the cloud, **real-world execution still needs local hardware**:\n\n- Jetson Orin kits run the trained models  \n- Sensors capture real-world data (vision, depth, orientation)  \n- Robot executes tasks in physical space\n\n> It’s like a student learning theory online but performing lab experiments in their home garage — cloud for practice, edge for execution.\n\n---\n\n## 3. Benefits of Cloud Labs\n\n- **Lower upfront cost:** no need to buy a powerful GPU immediately  \n- **Scalable computation:** increase GPU hours when needed  \n- **Remote access:** students can train and experiment from anywhere  \n\nThis model allows anyone with a **modest computer and an edge kit** to participate in advanced robotics learning.\n\n---\n\n## 4. Limitations to Consider\n\n- **Latency:** Real-time robot control from the cloud is not safe — delays can cause errors  \n- **Connectivity:** Requires a stable internet connection  \n- **Cloud cost:** Continuous use of GPUs adds up; careful management is necessary  \n\nThe solution is **train in the cloud, deploy locally** — best of both worlds.\n\n---\n\n## Key Resources\n\n- AWS RoboMaker: [https://aws.amazon.com/robomaker/](https://aws.amazon.com/robomaker/)  \n- NVIDIA Omniverse Cloud: [https://developer.nvidia.com/omniverse](https://developer.nvidia.com/omniverse)  \n- Remote ROS 2 Deployment: [https://docs.ros.org/en/rolling/Installation.html](https://docs.ros.org/en/rolling/Installation.html)  \n- Jetson Orin for Edge AI: [https://developer.nvidia.com/embedded/jetson-orin-nano](https://developer.nvidia.com/embedded/jetson-orin-nano)\n\n"
  },
  {
    "filename": "course-overview.md",
    "content": "---\nid: course-overview\ntitle: Course Overview\n---\n\n# Course Overview\n\nThis section provides an overview of the course content and structure."
  },
  {
    "filename": "hardware-requirements.md",
    "content": "---\nid: hardware-requirements\ntitle: Hardware Requirements\n---\n\n# Hardware Requirements\n\nThis section lists the recommended hardware for the course.\n\n# What You Need to Run Physical AI\n\nPhysical AI isn’t just about code — it’s about connecting **software, simulation, and real-world hardware**. To successfully train, test, and deploy humanoid robots, you need the right equipment. Think of it as building a **mini robotics ecosystem**.\n\n---\n\n## 1. Digital Twin Workstation — Simulation & Training HQ\n\nYour workstation is where heavy computation happens. It’s the **robot’s training gym**, letting you simulate physics, sensors, and AI safely.\n\n- **GPU:** NVIDIA RTX 4070 Ti (12 GB) or higher  \n  Needed for **graphics, physics, and AI inference** (Ray-tracing, USD assets).  \n- **CPU:** Intel i7 13th Gen or AMD Ryzen 9  \n  Handles physics calculations and sensor simulation.  \n- **RAM:** 64 GB DDR5 (minimum 32 GB)  \n  Ensures large simulations run without crashes.  \n- **OS:** Ubuntu 22.04 LTS  \n  ROS 2 runs natively here, making integration smooth.  \n\nWithout this, your robot is like an athlete **without a gym or training ground** — it can’t learn complex movements.\n\n---\n\n## 2. Physical AI Edge Kit — Robot “Brain & Senses”\n\nEven with a simulated robot, you need a **compact kit** to deploy AI in the real world. This acts as your robot’s **head**: brain + eyes + inner ear + voice interface.\n\n| Component | Role |\n|-----------|------|\n| Jetson Orin (Nano/NX) | Lightweight AI brain — runs inference for real-world deployment |\n| RealSense D435i / Depth Camera | Provides RGB + depth vision for perception & navigation |\n| USB IMU (or built-in) | Inner ear: tracks balance and orientation |\n| Microphone / Speaker Array | Enables voice command input and audio feedback |\n\nThis kit allows you to **test AI models on real hardware** before investing in full humanoid robots.\n\n---\n\n## 3. Robot Lab — Choose Your Level\n\nDepending on budget and goals, you can pick one of three approaches:\n\n- **Option A — Proxy (Budget-Friendly):** Quadruped or robotic arm (Unitree Go2 Edu)  \n  Affordable, durable, supports ROS 2. Perfect for learning basics.  \n- **Option B — Miniature Humanoid:** Table-top robots like Robotis OP3 or Hiwonder TonyPi Pro  \n  Small humanoid form, good for kinematics practice, limited AI performance.  \n- **Option C — Full Humanoid Lab:** High-end robots like Unitree G1  \n  Capable of full locomotion, precise manipulation, and SDK access.  \n\n> Think of it like learning to drive: Option A is a **simulator car**, Option B is a **toy car**, Option C is a **real vehicle** on the road.\n\n---\n\n## 4. Why Proper Hardware Matters\n\n- Simulation requires **high VRAM and CPU** — physics and AI models are heavy.  \n- Edge kits provide **hands-on, real-world deployment experience**.  \n- A well-equipped lab ensures you can **train, test, and iterate** safely and efficiently.  \n\nEven the best code is useless if the robot cannot perceive, move, or act in reality. Hardware is the bridge between **digital intelligence and physical embodiment**.\n\n---\n\n## Key Resources\n\n- [NVIDIA Isaac Sim System Requirements](https://developer.nvidia.com/isaac-sim)  \n- [Jetson Orin Developer Kit Documentation](https://developer.nvidia.com/embedded/jetson-orin-nano)  \n- [Intel RealSense Documentation](https://www.intelrealsense.com/developers/)  \n- ROS 2 Hardware Guides: [https://docs.ros.org/en/rolling/Hardware.html](https://docs.ros.org/en/rolling/Hardware.html)\n\n"
  },
  {
    "filename": "introduction.md",
    "content": "---\nid: introduction\ntitle: Introduction\n---\n\n# Welcome to the World of Physical AI\n\nImagine you created a super-smart AI — it can reason, understand language, make plans, and even predict outcomes. On a computer screen, it’s powerful. But it’s stuck. It cannot walk across a room to pick up a cup. It cannot push a button or open a door. Its intelligence is trapped inside a digital box.\n\nPhysical AI is about **unlocking that intelligence and giving it a body**. By combining AI with robotics, we bring intelligence into the real world — into a form that sees, moves, touches, and interacts. This is what humanoid robotics aims to achieve: AI that exists not just in simulation or code, but in a physical space, learning and acting like humans.\n\n---\n\n## The Core Idea: Embodied Intelligence\n\nThink of a pianist. You can give someone the sheet music (digital AI) and they can understand notes and rhythms. But until they sit at the piano and move their fingers, the music doesn’t exist in the world. Similarly, AI needs a **body to apply its knowledge**. The robot’s body, sensors, and actuators become the hands, eyes, ears, and muscles of your AI.\n\nHumanoid robots are especially interesting because our environments are designed for humans. Doors, chairs, stairs, and tools assume a human form. By giving AI a humanoid body, it can naturally interact with the world around it without reinventing every object and interface.\n\n---\n\n## What You’ll Explore in This Book\n\nBy following this course, you will gradually build the skills to create a humanoid robot capable of **seeing, moving, planning, and acting autonomously**. You’ll explore:\n\n- **Robotic Nervous Systems:** Learn how robots sense and act. Think of it as giving your robot a spine, nerves, and reflexes.  \n- **Digital Twins:** Build virtual worlds where robots can safely test movements, just like pilots use flight simulators.  \n- **AI Brains:** Teach your robot to perceive its surroundings, navigate spaces, and make intelligent decisions.  \n- **Vision-Language-Action (VLA):** Connect language understanding with physical actions, enabling your robot to execute instructions like a human assistant.\n\nIt’s like turning a video game character into a real-world helper. You write the code, and suddenly your creation walks, sees, and interacts outside the screen.\n\n---\n\n## Why Physical AI Matters\n\nHumans learn through interaction with the physical world — touching, observing, experimenting. Likewise, AI becomes more versatile and robust when it experiences reality. Physical AI has real-world applications such as:\n\n- **Healthcare:** Assistive humanoids helping elderly or disabled people.  \n- **Industrial Automation:** Robots performing tasks in factories, warehouses, or dangerous environments.  \n- **Disaster Response:** Robots navigating hazardous terrains where humans cannot safely go.  \n- **Education & Research:** Hands-on learning platforms for AI, robotics, and human-robot collaboration.\n\nWithout a physical body, AI can only simulate understanding. With a body, it **truly learns**, adapts, and contributes.\n\n---\n\nConsider a delivery drone. You can program it to follow GPS coordinates (digital AI), but until it’s actually flying and responding to wind, obstacles, and battery constraints, the intelligence is incomplete. Its \"body\" allows the AI to experience challenges it cannot foresee in simulation. Similarly, humanoid robots give AI a tangible body to understand and act in complex environments.\n\n---\n\n## What You’ll Achieve\n\nAfter completing this course, you will be able to:\n\n- Build a **robotic nervous system** connecting sensors, actuators, and AI.  \n- Simulate realistic environments using **Gazebo and Unity** to test robotic behavior safely.  \n- Develop perception, mapping, and navigation capabilities using **NVIDIA Isaac**.  \n- Create robots that understand natural language instructions and execute physical tasks.  \n- Apply AI in real-world humanoid robotics projects, bridging theory with tangible results.\n\nBy the end, your AI will not just **think** — it will **move, see, and act**.\n\n---\n\n## Key Resources for Hands-On Practice\n\nHere’s what you’ll need to get started with Physical AI:\n\n- **ROS 2 (Humble/Iron)** — for building robot control systems  \n  [Official ROS 2 Documentation](https://docs.ros.org/en/humble/index.html)  \n\n- **Gazebo Simulator** — for physics-based robot simulation  \n  [Gazebo Tutorials](http://gazebosim.org/tutorials)  \n\n- **Unity 3D** — for high-fidelity virtual environments  \n  [Unity Learn](https://learn.unity.com/)  \n\n- **NVIDIA Isaac Sim & Isaac ROS** — for AI perception and robotics pipelines  \n  [NVIDIA Isaac Documentation](https://developer.nvidia.com/isaac-sim)  \n\n- **OpenAI Whisper** — for speech-to-text capabilities in VLA modules  \n  [Whisper GitHub](https://github.com/openai/whisper)  \n\n- **Jetson Developer Kits** — Edge AI platform for real-world deployment  \n  [NVIDIA Jetson](https://developer.nvidia.com/embedded/jetson-developer-kits)  \n\nStart small — experiment with simulations first. Once you’re confident, deploy to physical robots or Jetson kits for real-world interaction.\n\n---\n\nThis chapter sets the foundation. Think of it as **learning the theory of flight** before building an airplane. In the following chapters, you’ll construct the **nervous system, virtual playgrounds, AI brains, and full humanoid integration** — step by step, building your robot from concept to reality.\n\n\n"
  },
  {
    "filename": "lab-architecture.md",
    "content": "---\nid: lab-architecture\ntitle: Lab Architecture\n---\n\n# Lab Architecture\n\nThis section describes the architecture of the lab environment.\n# How Everything Connects\n\nBuilding a physical AI lab is like **assembling a human body**: brain, senses, muscles, all coordinated to function smoothly. In robotics, these components map to **simulations, edge devices, sensors, and robots**.\n\n---\n\n## 1. Sim Rig — The Robot’s Gym\n\nThis is your **high-performance workstation** where your robot learns to move, see, and plan. It runs Gazebo, Unity, and NVIDIA Isaac Sim.\n\n- Performs **heavy physics simulations**  \n- Trains AI models for perception, navigation, and task planning  \n- Allows rapid iteration without risking hardware\n\nThink of it as a **robot’s training gym** — it practices virtually before stepping into the real world.\n\n---\n\n## 2. Edge Brain — Real-World AI Deployment\n\nThe Jetson kit serves as the **robot’s brain in the physical world**.\n\n- Runs trained AI models locally  \n- Processes sensor inputs in real time  \n- Sends commands to motors/actuators  \n\nIt’s like giving a trained athlete a **mind that can process situations instantly** — reacting and performing tasks safely.\n\n---\n\n## 3. Sensors — The Robot’s Eyes and Inner Ear\n\nSensors feed **real-time perception** to the Edge Brain.\n\n- **Cameras & LiDAR:** See the environment in 3D  \n- **IMU:** Tracks balance and orientation  \n- **Microphones:** Capture audio for voice commands\n\nWithout sensors, the robot is blind and deaf — like a human in total darkness and silence. These components allow **situational awareness and safe interaction**.\n\n---\n\n## 4. Actuators — The Robot’s Muscles\n\nActuators — whether a **full humanoid or proxy robot** — execute movements and tasks.\n\n- Follow commands from the Edge Brain  \n- Perform walking, grasping, and manipulation  \n- Translate virtual plans into physical action\n\nWithout muscles, even the smartest brain is useless. Actuators bring **intelligence to life**.\n\n---\n\n## 5. Putting It All Together\n\nYour lab should support **simulation → edge deployment → physical execution**.\n\n1. Train and test AI in the **Sim Rig**  \n2. Deploy trained models to the **Edge Brain**  \n3. Sense the environment using **cameras, IMU, microphones**  \n4. Execute actions via **actuators**  \n\n> Imagine training a gymnast virtually, then sending them into a real gym with real equipment — safe practice first, then real-world execution.\n\n---\n\n## Key Resources\n\n- NVIDIA Isaac Sim: [https://developer.nvidia.com/isaac-sim](https://developer.nvidia.com/isaac-sim)  \n- ROS 2 Edge Deployment Guides: [https://docs.ros.org/en/rolling/Hardware.html](https://docs.ros.org/en/rolling/Hardware.html)  \n- Jetson Orin Docs: [https://developer.nvidia.com/embedded/jetson-orin-nano](https://developer.nvidia.com/embedded/jetson-orin-nano)  \n- RealSense Sensors: [https://www.intelrealsense.com/developers/](https://www.intelrealsense.com/developers/)\n\n"
  },
  {
    "filename": "learning-outcomes.md",
    "content": "---\nid: learning-outcomes\ntitle: Learning Outcomes\n---\n\n# Learning Outcomes\n\nUpon completion of this course, students will be able to:\n\n# What You’ll Achieve by the End of This Course\n\nBy the end of this course, you won’t just have theoretical knowledge — you’ll **build, see, and control a humanoid robot**. Each module contributes to a set of tangible skills, preparing you for real-world robotics and Physical AI projects.\n\n---\n\n## Key Skills You Will Master\n\n- **Understanding Physical AI:** Learn why robots must operate in real environments, not just digital simulations.  \n- **ROS 2 — The Robot Nervous System:** Build nodes, topics, and actions to control sensors and actuators.  \n- **Simulation with Gazebo & Unity:** Safely test your robots in virtual worlds before touching hardware.  \n- **Perception & Navigation with NVIDIA Isaac:** Teach robots to see, map, and move intelligently.  \n- **Designing Humanoid Interactions:** Make robots perform human-like tasks and interact naturally.  \n- **Integrating GPT Models:** Add conversational intelligence, enabling robots to follow verbal instructions.  \n- **Capstone Development:** Deliver a working humanoid robot that can sense, plan, move, and act in real or simulated environments.\n\n---\n\n## Why These Outcomes Matter\n\n- You’ll **bridge the digital and physical worlds**, turning code into tangible actions.  \n- Your robot won’t just follow pre-programmed motions — it will **perceive, adapt, and respond**.  \n- These skills are **transferable to real-world robotics**, from industrial automation to home assistance.  \n\nImagine building a robot assistant that can bring a glass of water when asked — you’ll know the full pipeline from voice command → planning → navigation → grasp → delivery.\n\n---\n\n## Key Resources for Hands-On Learning\n\n- [ROS 2 Tutorials](https://docs.ros.org/en/rolling/Tutorials.html) — Learn how to create nodes, topics, services, and actions  \n- [Gazebo Simulation](http://gazebosim.org/tutorials) — Create physics-based simulations for safe testing  \n- [Unity Robotics Hub](https://unity.com/solutions/robotics) — Integrate robots with realistic virtual environments  \n- [NVIDIA Isaac Sim Documentation](https://developer.nvidia.com/isaac-sim) — Develop perception, navigation, and AI pipelines  \n- [OpenAI GPT API](https://platform.openai.com/docs/) — For integrating language understanding into robots\n\n"
  },
  {
    "filename": "module-1-ros2.md",
    "content": "---\nid: module-1-ros2\ntitle: \"Module 1: ROS2\"\n---\n\n# Module 1: ROS2\n\nThis module covers the fundamentals of ROS2.\n# Module 1: The Robot’s Nervous System\n\nIf robots were humans, ROS 2 would be their nervous system. It connects sensors like cameras and IMUs, actuators like motors, and the AI brain, allowing perception, decision-making, and action. Without this network, a robot is just metal and circuits — it can’t feel, respond, or move purposefully.\n\nThink of ROS 2 as the strings and levers of a mechanical puppet. Each string (node) can control one limb, while the puppet master (AI) decides what to do. Without the strings, the puppet can’t wave, point, or interact. With ROS 2, your robot moves, senses, and reacts in a coordinated way.\n\n---\n\n## What You Will Learn\n\n- **Nodes:** Like individual neurons, nodes perform one specific task — reading a sensor, processing data, or sending a command. By combining nodes, you create complex, coordinated behaviors. Imagine a small humanoid on your desk: one node watches for red balls, another moves the arm, and together they wave when a ball appears.  \n- **Topics:** These are the nerve fibers carrying information between nodes. A camera node sends images over a topic; a motor controller node subscribes to that topic and moves accordingly. It’s like neurons in your hand sensing a hot object and sending signals to your muscles to withdraw.  \n- **Services & Actions:** These are reflexes or long-running commands. Services are like asking, “Can you move your hand to X?” and getting a yes/no response. Actions let the robot execute a sequence of steps — “walk to the table while avoiding obstacles” — reporting progress along the way, similar to following a recipe step by step.  \n- **URDF (Unified Robot Description Format):** The blueprint of your robot. URDF defines links (bones), joints, and sensors. It ensures every joint moves correctly, sensors point the right way, and the robot respects physical limits — like an architect’s blueprint for a building.\n\n---\n\nAs you work through ROS 2, you will start small: connect a sensor to a node, read data, send it to another node controlling a motor. Then scale: make the robot respond to voice commands, walk across a room, or avoid obstacles. Each step reinforces the perception → decision → action cycle.\n\nImagine your robot sitting in front of you. A camera node detects a red ball. It sends the coordinates over a topic to the arm controller node. The arm executes a smooth wave, respecting the URDF constraints. The robot has just perceived, decided, and acted — all thanks to ROS 2.\n\n---\n\n## Why ROS 2 Matters\n\n- Modular design allows development of individual components independently. Vision, navigation, and motor control evolve separately but integrate seamlessly.  \n- Reusable across robots. Understanding ROS 2 lets you control robotic arms, drones, or full humanoids without starting from scratch.  \n- Clean separation of high-level AI logic from low-level control makes debugging and scaling easier.  \n\nOnce ROS 2 is mastered, your robot is no longer lifeless. It reacts intelligently to the environment, executing coordinated, purposeful movements.\n\n---\n\n## Key Resources\n\n- **ROS 2 Installation & Tutorials**  \n  [Official ROS 2 Docs](https://docs.ros.org/en/humble/index.html)  \n\n- **Python Client for ROS 2 (`rclpy`)**  \n  [Python ROS 2 Guide](https://docs.ros.org/en/humble/Concepts/About-Python-Client-Library.html)  \n\n- **URDF Basics**  \n  [URDF Tutorials](https://wiki.ros.org/urdf/Tutorials)  \n\n- **Example Projects to Explore:**  \n  - Waving arm using camera detection  \n  - Line-following robot using LiDAR  \n  - Obstacle avoidance with simulated sensors  \n\nBy completing this module, you build the **fundamental nervous system** of any robot — a foundation for simulation, perception, and humanoid autonomy.\n\n"
  },
  {
    "filename": "module-2-gazebo-unity.md",
    "content": "---\nid: module-2-gazebo-unity\ntitle: \"Module 2: Gazebo & Unity\"\n---\n\n# Module 2: Gazebo & Unity\n\nThis module compares and contrasts Gazebo and Unity for robotics simulation.\n# Your Robot’s Virtual Playground\n\nBefore sending your robot into the real world, it needs a **safe playground** — a place where mistakes don’t damage expensive hardware. That’s exactly what a **digital twin** provides: a virtual world that behaves like the real one.\n\nThink of it like training a pilot in a flight simulator. The pilot can crash, try daring maneuvers, and learn quickly — without risking lives. Similarly, your robot can stumble, bump into walls, or misjudge steps — all safely in simulation.\n\n---\n\n## Building a Realistic Virtual World\n\n- **Physics Simulation in Gazebo:** Gravity, friction, collisions — everything behaves realistically. Step on the floor? The robot’s legs respond. Drop an object? It falls naturally. Test walking, balancing, and manipulation safely.  \n- **URDF & SDF Robot Models:** Define your robot’s skeleton, joints, and sensors. Like building a Lego robot digitally, you can tweak limb lengths, joint limits, and sensor positions before touching the real hardware.  \n- **Simulated Sensors:** Cameras, LiDARs, depth sensors, IMUs — all act as if real. Your robot can see, hear, and measure the environment, learning perception and navigation safely.  \n- **Unity Integration:** Create visually rich environments — rooms, kitchens, or outdoor terrain. Test human-robot interactions and object manipulations as if the world were real.\n\n---\n\n## Learning by Doing: Real-World Examples\n\n- Navigate a room full of chairs and tables. If the robot bumps into something, the simulation shows exactly what happens — without breaking hardware.  \n- Teach a robot to pick up a cup on a table. Change the cup’s size, position, or weight in the simulation to let the robot adapt before trying it physically.  \n- Test humanoid walking on uneven terrain or stairs. Gazebo handles physics, Unity handles visuals — the robot gains realistic experience before hitting the real world.\n\n---\n\n## Why This Module Is Important\n\n- **Safety First:** Avoid expensive damage by testing virtually.  \n- **Fast Iteration:** Try, fail, adjust, and learn quickly.  \n- **Smooth Transition:** Robots trained in simulation behave predictably in real life.  \n- **Handle Complexity:** Tackle tight spaces, slippery floors, and moving obstacles without risk.  \n\nBy the end of this module, your robot will **walk, sense, and interact in a virtual world** as if it were real — a crucial step before moving to real hardware.\n\n---\n\n## Recommended Resources\n\n- **Gazebo Tutorials:**  \n  [Gazebo Official Docs](http://gazebosim.org/tutorials)\n\n- **Unity Robotics Hub:**  \n  [Unity Robotics](https://github.com/Unity-Technologies/Unity-Robotics-Hub)\n\n- **Simulated Sensor Examples:**  \n  - LiDAR mapping and obstacle detection  \n  - Depth camera for object recognition  \n  - IMU simulation for balance and orientation\n\n- **Sample Projects:**  \n  - Navigate a virtual room without collisions  \n  - Pick and place objects in Unity environment  \n  - Test humanoid walking and balancing routines  \n\nThis module ensures your robot **practices in a realistic yet safe environment**, preparing it perfectly for real-world deployment in the upcoming modules.\n\n"
  },
  {
    "filename": "module-3-isaac.md",
    "content": "---\nid: module-3-isaac\ntitle: \"Module 3: Isaac\"\n---\n\n# Module 3: Isaac\n\nThis module delves into NVIDIA Isaac for robotics development.\n# Giving Your Robot a Brain\n\nYour robot can move, but can it **see**, **understand**, or **decide** what to do? This module equips your robot with perception, navigation, and planning skills — essentially giving it a **brain**.\n\nThink of it like teaching a toddler: you don’t just let them walk blindly — you help them see obstacles, recognize objects, and make decisions about where to go or what to pick up. NVIDIA Isaac gives your robot this ability digitally, before it tries the real world.\n\n---\n\n## Building Vision and Awareness\n\n- **Isaac Sim:** A high-fidelity simulation environment that makes lighting, textures, and obstacles behave realistically. It’s like creating a digital sandbox where your robot can explore without risk.  \n- **Isaac ROS (VSLAM):** Lets your robot map its surroundings while moving. Imagine walking into a new room and mentally noting where tables, chairs, and doors are — VSLAM does this for the robot.  \n- **Nav2 Path Planning:** Enables the robot to navigate intelligently. Like humans planning a route around furniture, your robot plans paths to reach targets efficiently while avoiding obstacles.  \n\n---\n\n## Real-World Scenarios and Examples\n\n- Send your robot into a virtual kitchen to fetch a cup. It identifies obstacles, maps the space, and plans its path — without spilling anything or knocking objects over.  \n- Test bipedal walking on uneven surfaces. Isaac handles the physics, so the robot learns balance, stability, and foot placement.  \n- Integrate simulated sensors to detect doors, stairs, and objects in the environment. Your robot builds a mental map, learning to navigate complex spaces autonomously.\n\n---\n\n## Why This Module Is Critical\n\n- **Perception:** Robots can see, measure distances, and identify objects.  \n- **Navigation:** Robots can plan safe paths, avoid collisions, and reach goals.  \n- **Planning & Decision-Making:** Combines vision and movement into purposeful action.  \n- **Sim-to-Real Transfer:** Skills learned in simulation can be transferred to physical robots safely.\n\nBy the end of this module, your robot isn’t just a moving puppet — it has the **cognitive ability to sense, think, and plan** before executing actions in the real world.\n\n---\n\n## Key Resources for Hands-On Practice\n\n- **NVIDIA Isaac Sim Tutorials:**  \n  [Isaac Sim Docs](https://developer.nvidia.com/isaac-sim)  \n\n- **Isaac ROS Examples:**  \n  - Visual SLAM for navigation  \n  - Obstacle detection with depth cameras  \n  - Path planning using Nav2  \n\n- **Simulation Projects:**  \n  - Navigate cluttered virtual spaces  \n  - Pick and place objects autonomously  \n  - Integrate VSLAM for real-time mapping  \n\n> _Tip:_ Combine Gazebo simulations from Module 2 with Isaac Sim to test how perception integrates with motion planning in complex environments.\n\n"
  },
  {
    "filename": "module-4-vla.md",
    "content": "---\nid: module-4-vla\ntitle: \"Module 4: VLA\"\n---\n\n# Module 4: VLA\n\nThis module explores Visual-Language-Action (VLA) models in robotics.\n# Teaching Robots to Understand and Act\n\nRobots that move and see are impressive, but true intelligence requires **understanding human commands** and executing tasks autonomously. This module merges **vision, language, and action** to let your robot perform meaningful tasks.\n\nImagine telling a household assistant: “Bring me the red mug from the kitchen.” A regular robot might just ignore it or require step-by-step instructions. With VLA, your robot understands your words, plans a sequence of actions, and executes them safely.\n\n---\n\n## Core Components of VLA\n\n- **Voice-to-Action:** Using speech recognition (like OpenAI Whisper), your robot hears commands such as “Pick up the book” and converts them into actionable tasks.  \n- **Cognitive Planning (LLM Integration):** Turns natural language instructions into a step-by-step plan — e.g., navigate to bookshelf → identify object → grasp → return.  \n- **Perception & Object Recognition:** Integrates sensors (RGB, depth, LiDAR) to see objects, detect obstacles, and confirm tasks.  \n- **Autonomous Task Execution:** Combines all modules — planning, perception, and motion — to complete a goal entirely on its own.\n\n---\n\n## Real-World Examples\n\n- A service robot in a lab receives: “Bring me the green test tube.” It identifies the test tube, calculates a safe path around obstacles, picks it up, and delivers it.  \n- Warehouse automation: Robots interpret human verbal instructions, retrieve items from shelves, and avoid collisions with other robots or humans.  \n- Assisted living: Robots can understand requests like “Fetch my glasses” or “Water the plant,” interacting safely in human environments.\n\n---\n\n## Why VLA Is Game-Changing\n\n- Bridges **natural language** and **robotic action** — no coding needed for each specific task.  \n- Enables **multi-modal interaction** — voice, vision, and sensors work together seamlessly.  \n- Lays groundwork for **general-purpose robots** capable of understanding and acting in dynamic, human-centered environments.  \n- Makes robots more usable for people who aren’t robotics engineers — they just speak, and the robot executes.\n\nBy completing this module, your robot transitions from a programmed machine to a **flexible assistant** capable of understanding commands, planning actions, and performing tasks in the real world.\n\n---\n\n## Key Resources for Hands-On Practice\n\n- **OpenAI Whisper:** [Whisper GitHub](https://github.com/openai/whisper) — for voice recognition and transcription.  \n- **ROS 2 Action Servers:** For executing step-by-step tasks triggered by commands.  \n- **LLM Integration Examples:** Use GPT-style models to plan sequences of ROS 2 actions.  \n- **Simulation Projects:**  \n  - Command-driven pick-and-place tasks in Gazebo or Isaac Sim  \n  - Multi-step task execution: navigate → identify → manipulate → return  \n  - Object recognition pipelines with RGB-D cameras\n\n"
  },
  {
    "filename": "physical-ai-importance.md",
    "content": "---\nid: physical-ai-importance\ntitle: The Importance of Physical AI\n---\n\n# The Importance of Physical AI\n\nThis section discusses the significance of Physical AI.\n# Why Physical AI Is the Future\n\nAI on a screen is clever, but it can’t act in the real world. To **truly benefit humanity**—in homes, hospitals, factories, or disaster zones—AI must **move, sense, and make decisions in physical space**. That’s the essence of Physical AI.\n\nHumans don’t just think; we interact, touch, manipulate, and adapt. Similarly, AI reaches its full potential only when it can **embody intelligence** in a robot.\n\n---\n\n## Why Humanoid Robots?\n\n- **Familiar Form:** A humanoid can navigate stairs, open doors, use tools — environments designed for humans.  \n- **Learning from Humans:** By mimicking human motions and interactions, robots can leverage real-world data efficiently.  \n- **Universal Adaptability:** From lifting objects to helping in disaster relief, humanoid form allows robots to operate where humans operate.\n\nImagine a robot that could help elderly people at home. A four-legged robot might be stable, but only a humanoid can reach a shelf, press buttons, or hand you a cup.\n\n---\n\n## The Shift: From Digital to Embodied Intelligence\n\nThink of digital AI as playing chess on a computer—limited to abstract rules. Embodied AI is like **playing soccer in real life**—it has to react to dynamic environments, coordinate multiple senses, and make decisions under uncertainty.\n\nPhysical AI combines:\n\n- **Perception:** Seeing and understanding surroundings  \n- **Planning:** Deciding which actions to take  \n- **Action:** Executing movements safely and effectively  \n\nThis shift transforms AI from **virtual assistants** to **real-world helpers**, capable of making tangible impact.\n\n---\n\n## Key Takeaways\n\n- AI must leave the screen to interact with reality.  \n- Humanoid robots are uniquely suited to human environments.  \n- Physical AI integrates perception, planning, and action — bridging the gap between digital intelligence and the physical world.  \n- Embodied intelligence enables AI to assist, collaborate, and augment human capabilities in meaningful ways.\n\nBy understanding **why Physical AI matters**, you see that robotics is not just building machines — it’s creating intelligent agents that can participate in human life.\n\n---\n\n## Key Resources for Further Reading\n\n- [Embodied AI Papers](https://arxiv.org/search/?query=embodied+AI) — Research on AI in physical environments  \n- [Boston Dynamics Research](https://www.bostondynamics.com/) — Practical humanoid and legged robots  \n- [NVIDIA Isaac Sim Documentation](https://developer.nvidia.com/isaac-sim) — For simulating real-world robot interactions  \n- [ROS 2 Tutorials](https://docs.ros.org/en/rolling/Tutorials.html) — For robot control and sensor integration\n\n"
  },
  {
    "filename": "student-kit.md",
    "content": "---\nid: student-kit\ntitle: Student Kit\n---\n\n# Student Kit\n\nThis section details the contents of the student kit.\n# A Compact Robotics Kit to Get Started\n\nNot everyone can invest in a full humanoid robot. The **Economy Jetson Student Kit** gives you the essentials to **explore Physical AI and humanoid robotics** without breaking the bank.  \n\nThis kit lets you experiment with AI perception, navigation, and control in a hands-on way — your robot may not walk yet, but it can **see, think, and react**.\n\n---\n\n## Components of the Kit\n\n| Component | Price (approx) | Purpose |\n|-----------|----------------|---------|\n| Jetson Orin Nano / NX (8–16 GB) | ~$250–500 | Compact AI “brain” for robotics tasks |\n| Intel RealSense D435i Depth Camera | ~$349 | Gives robot “eyes”: RGB + depth perception |\n| USB Microphone / Speaker Array | ~$70 | Enables voice commands → robot listens and acts |\n| MicroSD Card, cables, power | ~$30 | Basic infrastructure for standalone robotics experiments |\n\n**Total Cost:** ~\\$700 (USD)\n\n> Think of this as a mini humanoid robot “head” with all the sensors and AI power — you can attach it to a small proxy robot, arm, or even a desk experiment.\n\n---\n\n## What You Can Do With It\n\n- Build simple perception pipelines: the robot can detect objects and measure distances  \n- Run AI models locally to practice navigation and path planning  \n- Integrate voice commands with robot actions, using small-scale Vision-Language-Action experiments  \n- Test and simulate ROS 2 nodes before scaling up to larger robots\n\n---\n\n## Why This Kit Matters\n\n- Affordable way to **get hands-on experience** with real-world robotics  \n- Understand the constraints of **embedded AI hardware** (memory, CPU/GPU limits)  \n- Provides a stepping stone toward **full humanoid robot labs** or **cloud-integrated simulations**\n\nEven without expensive hardware, you’ll gain **practical skills** that translate directly to larger robots and industrial setups.\n\n---\n\n## Key Resources\n\n- Jetson Orin Nano / NX: [https://developer.nvidia.com/embedded/jetson-orin-nano](https://developer.nvidia.com/embedded/jetson-orin-nano)  \n- Intel RealSense Cameras: [https://www.intelrealsense.com/](https://www.intelrealsense.com/)  \n- ROS 2 on Jetson: [https://docs.ros.org/en/rolling/Installation.html](https://docs.ros.org/en/rolling/Installation.html)  \n- Whisper API (voice commands): [https://openai.com/research/whisper](https://openai.com/research/whisper)  \n- MicroSD setup guide for Jetson: [https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit](https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit)\n\n"
  },
  {
    "filename": "weekly-breakdown.md",
    "content": "---\nid: weekly-breakdown\ntitle: Weekly Breakdown\n---\n\n# Weekly Breakdown\n\nThis section outlines the course content on a weekly basis.\n# Week-by-Week Roadmap\n\nThis course is structured to gradually take you from foundational concepts to building a fully functional humanoid robot. Each week introduces new skills while building on what you’ve learned before, ensuring a smooth progression from theory to hands-on implementation.\n\n---\n\n| Week | Focus | What You Learn / Do |\n|------|-------|---------------------|\n| 1–2 | Foundations of Physical AI | Understand the principles of **Physical AI** and embodied intelligence. Explore sensor types like LIDAR, cameras, and IMUs. Learn why humanoid robots are designed like humans. |\n| 3–5 | ROS 2 Fundamentals | Dive into **nodes, topics, and actions**. Build a simple “nervous system” for your robot. Understand packages, launch files, and parameter management. Start controlling a basic robot skeleton. |\n| 6–7 | Simulation with Gazebo | Set up your **digital twin** environment. Create URDF/SDF robot models. Add sensors and simulate physics like gravity, friction, and collisions. Learn Unity basics for visualization. |\n| 8–10 | AI Brain with NVIDIA Isaac | Develop perception, mapping, and navigation pipelines. Use **Isaac Sim** for realistic simulation and synthetic data. Implement VSLAM and reinforcement learning for robot motion. |\n| 11–12 | Humanoid Motion & Manipulation | Model humanoid kinematics, implement walking and balance control. Enable your robot to manipulate objects using hands. Fine-tune natural human-robot interaction. |\n| 13 | Conversational Robotics & Capstone | Integrate **GPT-based language understanding**. Convert voice commands into sequences of robot actions. Prepare and finalize your **capstone project**: a fully autonomous humanoid robot. |\n\n---\n\n## Why the Weekly Structure Works\n\nEach module is like a **level in a game**. You start with basics, unlock skills, and gradually tackle more complex challenges:\n\n- Foundations let you understand what makes robots “alive.”  \n- ROS 2 modules teach the robot to sense and act — its nervous system.  \n- Simulations give a **risk-free playground** to test behaviors.  \n- NVIDIA Isaac adds perception and decision-making — the robot starts thinking.  \n- Humanoid motion ensures the robot **walks, balances, and interacts naturally**.  \n- Finally, conversational robotics bridges human language and robot action — your robot responds like a real assistant.  \n\nBy the end, you’ll have **step-by-step experience** turning concepts into a robot that can move, see, think, and act.\n\n---\n\n## Key Resources\n\n- [ROS 2 Tutorials](https://docs.ros.org/en/rolling/Tutorials.html) — Node and topic examples  \n- [Gazebo Tutorials](http://gazebosim.org/tutorials) — Physics and sensor simulation  \n- [Unity Robotics Hub](https://unity.com/solutions/robotics) — Integrate simulations with visuals  \n- [NVIDIA Isaac Sim Docs](https://developer.nvidia.com/isaac-sim) — AI perception and navigation pipelines  \n- [OpenAI GPT API](https://platform.openai.com/docs/) — Voice-to-action integration\n\n"
  }
]